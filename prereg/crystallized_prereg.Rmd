---
title           : "A Taxonomy of Data Synthesis: A Tutorial"
shorttitle      : "A Taxonomy of Data Synthesis"
date            : "`r Sys.setlocale('LC_TIME', 'C'); format(Sys.time(), '%d\\\\ %B %Y')`"
author: 
  - name        : Emorie D. Beck
    affiliation : Feinberg School of Medicine
  - name        : Emily C. Willroth
    affiliation : Feinberg School of Medicine
  - name        : Daniel K. Mroczek
    affiliation : Feinberg School of Medicine, Northwestern University
  - name        : Eileen K. Graham
    affiliation : Feinberg School of Medicine
output: 
  
  html_document:
    theme: united
    highlight: tango
    df_print: paged
    code_folding: show
    toc: true
    toc_float: true
bibliography: r-references.bib
biblio-style: "apalike"
csl: "apa-6th-edition.csl"
editor_options: 
  chunk_output_type: console
---

```{r, echo = F, warning = F, message = F, results = 'hide'}
knitr::opts_chunk$set(echo = F, message = F, warning = F, error = F)
options(knitr.kable.NA = '')
library(knitr)
library(kableExtra)
library(plyr)
library(tidyverse)

wd <- "/Users/edb5662/Library/CloudStorage/Box-Box/emorie post doc/01_projects/02_data synthesis/01_crystallized"
```


# Study Information

## Title
<!-- Provide the working title of your study. It may be the same title that you submit for publication of your final manuscript, but it is not a requirement. The title should be a specific and informative description of a project. Vague titles such as 'Fruit fly preregistration plan' are not appropriate.

Example: Effect of sugar on brownie tastiness. -->

`r rmarkdown::metadata$title`


## Description
<!-- Please give a brief description of your study, including some background, the purpose of the of the study, or broad research questions. The description should be no longer than the length of an abstract. It can give some context for the proposed study, but great detail is not needed here for your preregistration.

Example: Though there is strong evidence to suggest that sugar affects taste preferences, the effect has never been demonstrated in brownies. Therefore, we will measure taste preference for four different levels of sugar concentration in a standard brownie recipe to determine if the effect exists in this pastry. -->

A key part of the scientific enterprise involves establishing robust, replicable, and generalizable relationships among diverse phenomena. For the better part of a century, meta-analytic techniques, in which effect sizes of relationships among phenomena are pulled from the published or unpublished literature and statistically pooled, have been the cornerstone of testing the robustness and generalizability. However, as more data become publicly available, it is becoming increasingly easy and important to synthesize data sources, rather than just results of those sources. In response, a number of new techniques, including pooled analyses, individual participant meta-analyses, and coordinated analyses, have emerged to synthesize such diverse data sources.  

Despite the promises of each of these techniques, there has been little to no systematic review of the methods available or how to carry them out. As a result, many researchers are unaware of the wealth of methods available for data synthesis. But understanding what methods are available and how to best carry them out is critical for guiding future research using different data synthesis techniques. The present study aims to fill this gap.  

In addition, in recent years, the links between personality and cognitive ability and their links to aging have become increasingly popular as researchers look to use them to understand how aging processes unfold. However, less research has looked at links between personality and specific domains of cognitive functioning, particularly in a multi-study format, which is critical to understand how the interplay among personality, cognitive function, and aging unfold in a more nuanced manner.  

To demonstrate how to conduct a variety of data synthesis techniques, as well as their utility and challenges, the proposed study investigates whether the Big Five prospectively predicts crystallized / knowledge domain of cognitive ability in 13 longitudinal panel studies. Because of the many options available for synthesizing the data from these studies to test the association, we will detail five broad data synthesis methods: (1) pooled analysis of individual participant data (IPD), (2) pooled analysis of individual participant data (IPD) using dummy codes or random effects, (3) coordinated analyses followed by random effects meta-analysis, (4)  coordinated analyses reported together, and (5) traditional meta-analysis of effect sizes from the published and unpublished literature. In addition, we will demonstrate how to carry out four of these five methods (excluding traditional meta-analyses).  

Each of these methods will be explained in more detail later, but key features and differences across methods are summarized in Table 1 below.  

```{r}
sprintf("%s/codebooks/crystallized_tables.xlsx", wd) %>%
  readxl::read_xlsx(., sheet = "Table 1") %>%
  select(-ModNum) %>%
  kable(.
        , "html"
        , align = c("r", "c", "l", "l", "c", "l")
        , caption = "<strong>Table 1</strong><br><em>Key Features of Five Levels of Data Synthesis</em>") %>%
  kable_styling(full_width = F) %>%
  kableExtra::group_rows("Single Model", 1, 2) %>%
  kableExtra::group_rows("Multiple Models", 3, 5) 
```

## Research Question  
<!-- List specific, concise, and testable hypotheses. Please state if the hypotheses are directional or non-directional. If directional, state the direction. A predicted effect is also appropriate here. If a specific interaction or moderation is important to your research, you can list that as a separate hypothesis.

Example: If taste affects preference, then mean preference indices will be higher with higher concentrations of sugar. -->
1. Does personality (the Big Five) predict crystallized / knowledge abilities?  

2. Do estimates of personality-crystallized / knowledge domain relationships vary across studies?   

The previous literature of the Big Five predicting crystallized / knowledge domains is mixed. The most frequent association is a positive association between the Openness / Intellect domain and crystallized / knowledge domains [@rammstedt2018relationships; @goff1992personality; @ashton2000fluid; @stankov2018low]. Despite this, a number of studies have found no relation or only or stronger relations at the facet level [@beauducel2007impact; @rammstedt2018relationships].  

Outside of Openness, there is some evidence that Conscientiousness and Neuroticism are negatively associated with crystallized / knowledge domains [@rammstedt2018relationships; @goff1992personality]. However, while some studies show negative relationships between Extraversion and crystallized / knowledge domains [@rammstedt2018relationships], others show no association between the two [@goff1992personality].  

Thus, we expect Openness to be a relatively robust predictor of crystallized / knowledge domains across studies, but largely expect weak or null estimates of associations with other personality domains.  

# Design Plan
<!-- In this section, you will be asked to describe the overall design of your study. Remember that this research plan is designed to register a single study, so if you have multiple experimental designs, please complete a separate preregistration. -->


## Study type

**Observational Study**. Data is collected from study subjects that are not randomly assigned to a treatment. This includes surveys, natural experiments, and regression discontinuity designs.  

## Study design
<!-- Describe your study design. Examples include two-group, factorial, randomized block, and repeated measures. Is it a between (unpaired), within-subject (paired), or mixed design? Describe any counterbalancing required. Typical study designs for observation studies include cohort, cross sectional, and case-control studies.

This question has a variety of possible answers. The key is for a researcher to be as detailed as is necessary given the specifics of their design. Be careful to determine if every parameter has been specified in the description of the study design. There may be some overlap between this question and the following questions. That is OK, as long as sufficient detail is given in one of the areas to provide all of the requested information. For example, if the study design describes a complete factorial, 2 X 3 design and the treatments and levels are specified previously, you do not have to repeat that information.

Example: We have a between subjects design with 1 factor (sugar by mass) with 4 levels. -->

The present study plans to use 13 existing longitudinal studies that include personality, measures of cognitive ability, and background covariates. More detail on each study can be found below. Importantly, at the time of this preregistration, we are waiting for permission to use data from several of the studies. This has two consequences. First, if access is not granted, we will not be able to include these studies. Second, until access is granted, we will not have access to the full data dictionaries, including variable names, scales, etc. However, the list of data requested are included in this preregistration. Their cleaning, compositing, and inclusion rules and procedure have been detailed as much as possible and will mirror the harmonization rules in all other studies.  

### Berlin Aging Study (BASE-I)  

The Berlin Aging Study (I) is a longitudinal interdisciplinary study of aging that began in 1990. The data are available, by application, from https://www.base-berlin.mpg.de/en/system/files/media/pdf/a97295f241d06bf496b5eeae4251a34e/base-data-transfer_form_online.pdf.  

Participants included 516 individuals over 70, stratified on age and gender, recruited from a sample of the Berlin City Registry. To date, most participants are deceased, with most attrition from the study due to mortality or severe health complications. Since 1990, there have been seven follow-up measurement occasions (8 total).  

Sample sizes vary over time, from 516 (T1) to 23 (T8). To ensure adequate sample sizes, the present study will use data up to the fourth wave, which included 164 participants with full data. This provides 99% power to detect a correlation effect size of ~.32, two-tailed alpha at .05.  

### National Longitudinal Study of Youth 1979 - Child and Young Adult (CNLSY-79)  

The Children to Young Adults Study (CNLSY; Bureau of Labor Statistics, 2017) is an offshoot study of the National Longitudinal Study of Youth (NLSY79), which is an ongoing longitudinal, nationally representative study in the United States. These data are available on the National Bureau of Labour Statistics website dedicated to the NLSY studies by creating a free account (https://www.nlsinfo.org/investigator/pages/login).  

Participants included more than 12,500 individuals in the United States that began in 1979. The CNLSY includes the biological children of the NLSY79 participants and began in 1986. Children (10 years and older) completed separate inventories from children (or “young adults”) aged 15 and above. Mothers of children 10 and below also completed surveys on the children prior to age 10. All participants were interviewed in addition to surveys.  

Sample sizes vary by year, ranging from approximately 1,331 (1979) to 11,530 (2016). This provides 99\% power to detect a zero-order correlation effect size of ~.05.  

### Einstein Aging Study (EAS)  
The Einstein Aging Study is an ongoing longitudinal study of older adults in the United States that began in 1980. The data are available on a project-by-project basis by submitting a concept proposal at https://www.einstein.yu.edu/departments/neurology/clinical-research-program/eas/data-sharing.aspx.  

Since 1993, the EAS has systematically recruited a representative aging sample in the Bronx, New York. As of 2017, 2,600 participants were enrolled in the study. As of 2010, approximately 200 of the enrolled participants had autopsy data. More information on the study can be found at http://www.einstein.yu.edu/departments/neurology/clinical-research-program/EAS/.  

Sample sizes vary over time, with ranges across waves not publicly available. However, we suspect approximately 2,000 participants to have basic personality and cognitive ability data. This yields 99\% power to detect a zero-order correlation effect size of .10, two-tailed at alpha .05.    

### German Socioeconomic Panel (GSOEP)  

The German Socioeconomic Panel Study (GSOEP; Socio-Economic Panel, 2017) is an ongoing longitudinal study of Germans collected by the German Institute of Economic Research (DIW Berlin). The data are freely available at https://www.diw.de/soep by application.  

Data have been collected annually since 1984 (the latest data release includes data up to 2017). Participants have been recruited from more than 11,000 households, which are nationally representative of private German households. 20,000 individuals are sampled each year, on average. It is critical to note that the GSOEP samples households, not individuals, and the households consist of individuals living in both the “old” and “new” federal states (the former West and East Germany), foreigners, and recent immigrants to Germany.  

 Sample size varies by year, ranging from approximately 10,000 (1989) to 31,000 (2013). This provides 99\% power to detect a zero-order correlation effect size of ~.06, two-tailed at alpha < .05.  
 
### Household, Income, and Labour Dynamics in Australia (HILDA)  

The Household Income and Labour Dynamics in Australia (HILDA; Wilkins, Laß, Butterworth, & Vera-Toscano, 2019) study is an ongoing longitudinal study of Australian households. These data are available through application from https://melbourneinstitute.unimelb.edu.au/hilda/for-data-users.  

Participants were recruited from more than 17,000 individuals. Data have been collected annually since 2001. The latest data release includes 17 waves of data from 2001 to 2017. More documentation can be found in the HILDA data dictionary at https://www.online.fbe.unimelb.edu.au/HILDAodd/srchSubjectAreas.aspx.  

Sample sizes vary by year, ranging from 12,408 (2004) to 17,693 (2016). This provides 99\% power to detect a zero-order correlation effect size of ~.03, two tailed at alpha .05.  

### Health and Retirement Study (HRS)  
The Health and Retirement Study [HRS; @juster1995overview] is an ongoing longitudinal study of households in the United States. These data are available at https://hrs.isr.umich.edu by creating a free account.  

Participants were recruited from more than 35,000 individuals from the financial households of individuals born between 1931 and 1941 in the US. Data have been collected biannually since 1992. The latest data release includes data up to 2016. On average, 10,000 individuals are sampled each wave More information on the HRS can be found at https://hrs.isr.umich.edu/documentation/survey-design, but, in short, the HRS is a nationally representative sample of adults over 50 in the US. It is critical to note that the HRS samples households of the original cohort and follows individuals and their spouses or partners until their death.  

Sample size varies by year, ranging from approximately 7,500 (2014) to 15,500 (1992). (https://hrs.isr.umich.edu/sites/default/files/biblio/ResponseRates_2017.pdf). This provides 99% power to detect a zero-order correlation effect size of ~.04, two-tailed at alpha .05.  

### Longitudinal Aging Study Amsterdam (LASA)  
The Longitudinal Aging Study Amsterdam is an ongoing longitudinal study that began in 1992. These data are available, through application at https://www.lasa-vu.nl/data/availability_data/availability_data.htm.

Participants who were between 55 and 84 years at the start of the study were recruited from the NESTOR study on Living Arrangements and Social Networks of older adults, which was randomly selected from municipality registers in 1992, with an oversampling of the oldest old and men. Data are collected approximately every three years (1992, 1995, 1998, 2001, 2005, 2008, 2011, 2015, 2018). Additional cohorts are introduced every 10 years, with additional participants (Cohorts 2 and 3) recruited in 2002 and 2012. Additional information and documentation are available at https://www.lasa-vu.nl/index.htm.  

Sample sizes vary by year, ranging from 1522 (Wave H; 763 Cohort 1, 759 Cohort 2) to 3107 (Wave B, Cohort 1). This provides 99\% power to detect a zero-order correlation effect size of ~.10, two-tailed at alpha .05. 

### RUSH Memory and and Aging Project (MAP)  
The RUSH Memory and Aging Project (MAP) is an ongoing longitudinal study that began in 1997 [@a2012overview]. These data are available, through application from https://www.radc.rush.edu/requests.htm.  

Participants who were 65 and older were recruited from retirement communities and subsidized senior housing facilities throughout Chicagoland and northeastern Illinois beginning in 1997. Data are collected annually, and all participants are organ donors. Additional participants are recuited each year. Additional information and documentation on the data can be found at https://www.radc.rush.edu/docs/var/variables.htm.  

Sample sizes vary by year, ranging from 52 (1997) to 2205 participants. This provides 99\% power to detect a zero-order correlation effect size of ~.09, two-tailed at alpha .05.  

### RUSH Minority Aging Reseach Study (MARS)  
The RUSH Minory Aging Research Study (MARS) is an ongoing longitudinal study that began in 2004. These data are available, through application, from https://www.radc.rush.edu/requests.htm.  

Participants were Black individuals 65 and older who were recruited from community locations in the Chicago Metropolitan area and suburbs beginning in 2004. Data are collected annually. Additional participants are recruited each year. Additional information and documentation on the data can be found at https://www.radc.rush.edu/docs/var/variables.htm.  

Sample sizes vary by year, ranging from 80 (2004) to 790 (2020). This provides 99% power to detect a zero-order correlation effect size of ~.17, two-tailed at alpha .05.  

### RUSH Religious Orders Study (ROS)  
The RUSH Religious Orders Study (ROS) is an ongoing longitudinal study that began in 1994 [@a2012overview]. These data are available, through application from https://www.radc.rush.edu/requests.htm. 

Older (65 and above) Catholic nuns, priests, and brothers with no prior dementia diagnosis and who agreed to annual evaluations and eventual organ donation were recruited from more than 40 groups across the United States. Additional participants are recuited each year. Additional information and documentation on the data can be found at https://www.radc.rush.edu/docs/var/variables.htm.  

Sample sizes vary bt year from 353 participants (1994) to 1487 participants, including 797 deceased participants with autopsy data (2019, 2020). This provides 99\% power to detect a zero-order correlation effect size of ~.11, two-tailed at alpha .05.  

### Origins of the Variances of the Oldest-Old: Octogenarian Twins (OCTO-TWIN)  
The Origins of the Variances of the Oldest-Old: Octogenarian Twins (OCTO-TWIN) study was a longitudinal study of twin-pairs in Sweden born before 1913 that began in 1991. Data are available, by application, from the study leaders at the Karolinska Institute.  

Twin-pairs from the Swedish Twin registry who were born before 1913 were invited to be part of the study in 1991. Additional follow-ups were collected in 1993, 1995, 1997, and 1999 on all surviving twins. The initial sample included 351 twin pairs (149 monozygotic and 202 same-sex dizygotic pairs). More information on the study can be found at https://webcache.googleusercontent.com/search?q=cache:HcheF2l9zc0J:https://psy.gu.se/digitalAssets/1469/1469717_octo-twin-brief-presentation.pdf+&cd=1&hl=en&ct=clnk&gl=us&client=safari.  

Sample sizes vary by year, from 222 (wave 5; 43 pairs) to 702 (wave 1; 351 pairs), which yields 99% power to detect a correlation effect size of ~.19, two-tailed at alpha .05.  

### Swedish Adoption Twin Study of Aging (SATSA)  

The Swedish Adoption Twin Study of Aging (SATSA) is a longitudinal study of twin pairs from the Swedish Twin Registry that began in 1984. Data are available through the ICPSR database at https://www.icpsr.umich.edu/web/ICPSR/studies/3843.  

All twin-pairs on the Swedish Twin Registry who were separated at an early age were invited to be a part of the study in 1984. A control sample of twins reared together were also included. Additional waves of all participants were collected in 1987, 1990, 1993, 2004, 2007, 2010, 2012, and 2014. More information, including codebooks, scales, and variable search functions can be found at https://www.maelstrom-research.org/mica/individual-study/satsa/#.

Sample sizes vary by wave, ranging from 2018 participants at baseline (1984) to 379 participants (IPT7). Given that the target measures were collected at baseline, this provides 99\% power to detect a zero-order correlation effect size of ~.10, two-tailed at alpha .05. 

### Seattle Longitudinal Study (SLS)  
The Seattle Longitudinal Study is an ongoing longitudinal study of adult Seattle residents that began in 1956. Data are available, by request from https://sls.psychiatry.uw.edu/researchers/public-access-data-sets/.

The first cohort of participants were recruited from HMO plan members of the Group Health Cooperative of Puget Sound in Seattle. Seven years later, the first cohort and a new cohort of similarly aged participants were solicited again. Over the next half century, this procedure was repeated every seven years and is ongoing. More information on the measures collected can be found at https://sls.psychiatry.uw.edu/researchers/measures/. 

Sample sizes vary by year, from 302 (1956) to 421 (2005). This provides 99\% power to detect correlation effect sizes of ~.23, two-tailed alpha at .05.  

# Sampling Plan
<!-- In this section we’ll ask you to describe how you plan to collect samples, as well as the number of samples you plan to collect and your rationale for this decision. Please keep in mind that the data described in this section should be the actual data used for analysis, so if you are using a subset of a larger dataset, please describe the subset that will actually be used in your study. -->


## Existing data
<!-- Preregistration is designed to make clear the distinction between confirmatory tests, specified prior to seeing the data, and exploratory analyses conducted after observing the data. Therefore, creating a research plan in which existing data will be used presents unique challenges. Please select the description that best describes your situation. Please do not hesitate to contact us if you have questions about how to answer this question (prereg@cos.io). -->

**Registration prior to accessing the data**. As of the date of submission, the data exist, but have not been accessed by you or your collaborators. Commonly, this includes data that has been collected by another researcher or institution.

**Registration prior to analysis of the data**. As of the date of submission, the data exist and you have accessed it, though no analysis has been conducted related to the research plan (including calculation of summary statistics). A common situation for this scenario when a large dataset exists that is used for many different studies over time, or when a data set is randomly split into a sample for exploratory analyses, and the other section of data is reserved for later confirmatory data analysis.

## Explanation of existing data
<!-- If you indicate that you will be using some data that already exist in this study, please describe the steps you have taken to assure that you are unaware of any patterns or summary statistics in the data. This may include an explanation of how access to the data has been limited, who has observed the data, or how you have avoided observing any analysis of the specific data you will use in your study.

An appropriate instance of using existing data would be collecting a sample size much larger than is required for the study, using a small portion of it to conduct exploratory analysis, and then registering one particular analysis that showed promising results. After registration, conduct the specified analysis on that part of the dataset that had not been investigated by the researcher up to that point.

Example: An appropriate instance of using existing data would be collecting a sample size much larger than is required for the study, using a small portion of it to conduct exploratory analysis, and then registering one particular analysis that showed promising results. After registration, conduct the specified analysis on that part of the dataset that had not been investigated by the researcher up to that point. -->

The first author, who will be conducting all analyses, has previously worked with data from the National Longitudinal Study of Youth, Child and Young Adult (CNSLY-79), the German Socioeconomic Panel Study (GSOEP), the Household, Income, and Labour Dynamics in Australia Study (HILDA), and the Health and Retirement Study (HRS), including key personality and cognition data. However, she has not previously linked these data directly (cross-sectionally or longitudinally) nor has she previously worked with or accessed the Memory and Aging Project (MAP), the Religious Orders Study (ROS), the Swedish Adoption Twin Study of Aging (SATSA), the Berlin Aging Study (BASE), the Einstein Aging Study (EAS), the Longitudinal Aging Study Amsterdam (LASA), the Minority Aging Research Study (MARS), the Origins of the Oldest Old: Octogenarian Twins (OCTO_TWIN), or the Seattle Longitudinal Study (SLS) data. All core variables, data cleaning procedures, and analytic procedures for each of these studies are included with the present preregistration, where possible.  

## Sample size rationale
<!-- This could include a power analysis or an arbitrary constraint such as time, money, or personnel. This gives you an opportunity to specifically state how the sample size will be determined. A wide range of possible answers is acceptable; remember that transparency is more important than principled justifications. If you state any reason for a sample size upfront, it is better than stating no reason and leaving the reader to "fill in the blanks." Acceptable rationales include: a power analysis, an arbitrary number of subjects, or a number based on time or monetary constraints.

Example: We used the software program G*Power to conduct a power analysis. Our goal was to obtain .95 power to detect a medium effect size of .25 at the standard .05 alpha error probability. -->

Sample sizes vary across study and waves (see above). But each of these studies, with the exception of the Seattle Longitudinal Study, have at least 500 participants at the target wave, which provides 99% power to detect correlations of at least .19, two-tailed alpha at .05. Many of these studies have considerably more participants, and the overall estimates will have considerably higher power.  

## Stopping rule
<!-- If your data collection procedures do not give you full control over your exact sample size, specify how you will decide when to terminate your data collection. 

You may specify a stopping rule based on p-values only in the specific case of sequential analyses with pre-specified checkpoints, alphas levels, and stopping rules. Unacceptable rationales include stopping based on p-values if checkpoints and stopping rules are not specified. If you have control over your sample size, then including a stopping rule is not necessary, though it must be clear in this question or a previous question how an exact sample size is attained.

Example: We will post participant sign-up slots by week on the preceding Friday night, with 20 spots posted per week. We will post 20 new slots each week if, on that Friday night, we are below 320 participants. -->

The authors did not collect these data and most of these studies are still enrolling participants. Sample sizes will be determined by data sent to the authors shortly after preregistration.  

# Variables
<!-- In this section you can describe all variables (both manipulated and measured variables) that will later be used in your confirmatory analysis plan. In your analysis plan, you will have the opportunity to describe how each variable will be used. If you have variables which you are measuring for exploratory analyses, you are not required to list them, though you are permitted to do so. -->


## Measured variables
<!-- Describe each variable that you will measure. This will include outcome measures, as well as any predictors or covariates that you will measure. You do not need to include any variables that you plan on collecting if they are not going to be included in the confirmatory analyses of this study.

Observational studies and meta-analyses will include only measured variables. As with the previous questions, the answers here must be precise. For example, 'intelligence,' 'accuracy,' 'aggression,' and 'color' are too vague. Acceptable alternatives could be 'IQ as measured by Wechsler Adult Intelligence Scale' 'percent correct,' 'number of threat displays,' and 'percent reflectance at 400 nm.'

Example: The single outcome variable will be the perceived tastiness of the single brownie each participant will eat. We will measure this by asking participants ‘How much did you enjoy eating the brownie’ (on a scale of 1-7, 1 being 'not at all', 7 being 'a great deal') and 'How good did the brownie taste' (on a scale of 1-7, 1 being 'very bad', 7 being 'very good'). -->

There will three categories of data:  

**(1) Personality** will be measured using Big Five Personality scales.  

**(2) Crystallized / Knowledge Cognitive Ability** will be measured using a variety of measures (see Table 3):  
* Vocabulary (6)  
* Information Tests (3)  
* The Boston Naming Test (4)  
* Reading Comprehension (1)  
* Word Pronunciation (1)  
* Spot a Word (1)  

**(3) Background variables**:  

* age (numeric)  
* gender (0 = male, 1 = female)  
* education (0 = high school or below; 1 = college, 2 = higher degree)  
* self-rated health (Likert like scale)  

A summary of the personality variables are in Table 2, and which measures were available in each study are in Table 3.  

```{r}
st <- seq(from = 1, by = 13, length.out = 5); end <- st + 12
sprintf("%s/codebooks/crystallized_tables.xlsx", wd) %>%
  readxl::read_xlsx(., sheet = "Table 2") %>%
  select(-Measure, -Used, -p_item) %>%
  kable(.
        , "html"
        , caption = "<strong>Table 2</strong><br><em>Personality Measures and Scales Across Studies</em>") %>%
  kable_styling(full_width = F) %>%
  kableExtra::group_rows("Extraversion",           st[1], end[1]) %>%
  kableExtra::group_rows("Agreeableness",          st[2], end[2]) %>%
  kableExtra::group_rows("Conscientiousness",      st[3], end[3]) %>%
  kableExtra::group_rows("Neuroticism",            st[4], end[4]) %>%
  kableExtra::group_rows("Openness",               st[5], end[5])#%>% 
  # kableExtra::save_kable(., file = sprintf("%s/preregistration/table_1.html", wd))
```

```{r}
sprintf("%s/codebooks/crystallized_tables.xlsx", wd) %>%
  readxl::read_xlsx(., sheet = "Table 3") %>%
  select(-Category, -`...17`) %>%
  kable(.
        , "html"
        , align = c("r","r", rep("c", 13))
        , caption = "<strong>Table 3</strong><br><em>Personality, Cognitive Domain, and Background Measures Across Studies</em>") %>%
  kable_styling(full_width = F) %>%
  collapse_rows(1, "top") %>%
  kableExtra::group_rows("Personality", 1, 5) %>%
  kableExtra::group_rows("Cognitive Tests", 6, 10) %>%
  kableExtra::group_rows("Background Covariates", 11, 14)
```

More detail on the scale, recoding, and summarizing of these data are available in the codebook attached with this preregistration.  


## Indices
<!-- If any measurements are  going to be combined into an index (or even a mean), what measures will you use and how will they be combined? Include either a formula or a precise description of your method. If your are using a more complicated statistical method to combine measures (e.g. a factor analysis), you can note that here but describe the exact method in the analysis plan section.

If you are using multiple pieces of data to construct a single variable, how will this occur? Both the data that are included and the formula or weights for each measure must be specified. Standard summary statistics, such as "means" do not require a formula, though more complicated indices require either the exact formula or, if it is an established index in the field, the index must be unambiguously defined. For example, "biodiversity index" is too broad, whereas "Shannon’s biodiversity index" is appropriate.

Example: We will take the mean of the two questions above to create a single measure of 'brownie enjoyment.'  -->

Personality and cognitive ability will be composited into trait/composite scores preserving the original scale (personality) or their respective domains as specified in the preregistered codebook (cognitive ability) in each data set. Then, personality and cognitive ability domain scores in each data set will be converted to Percentages Of the Maximum Possible score (POMP) in the mega-analytic procedure [@cohen1999problem]. Unlike standardization procedures, that have a mean of zero and unit variance and can be misleading when data are skewed, POMP does not rescale sample variance based on the observed data, which overly relies on deviations from the mean. Instead, POMP relies on the ratio between the difference between a score and the minimum and the maximum and minimum, or  

POMP = $\frac{observed-minimum}{maximum-minimum}$*10.  

More information on the exact scales used to create the variables are in the table below.  

# Analysis Plan
<!-- You may describe one or more confirmatory analysis in this preregistration. Please remember that all analyses specified below must be reported in the final article, and any additional analyses must be noted as exploratory or hypothesis generating.

A confirmatory analysis plan must state up front which variables are predictors (independent) and which are the outcomes (dependent), otherwise it is an exploratory analysis. You are allowed to describe any exploratory work here, but a clear confirmatory analysis is required. -->

The analysis plan will be broken into parts to answer different questions associated with this study.  

Following pre-registration of this study, all data will be downloaded from study websites or received directly from data maintainers for each study. All target variables, as well as their rescaling and cleaning procedures, have been preregistered with this study.  

After all data are received, data for each study will be cleaned separately by the first author. These cleaning procedures can be roughly broken into pulling, rescaling, and/or compositing (1) background variables at baseline, (2) baseline personality, (3) crystallized / knowledge cognitive ability at follow-up.  

Once each data set is prepared, data sets across studies will be combined and scales will be harmonized as needed (e.g., POMP for personality data will be calculated separately for each test due to differences in data missingness across models).  

Once data are combined, we will address the three main research questions, all of which will use the `brms` package in `R`.  

**1. Does personality (the Big Five) predict crystallized / knowledge domain abilities?**  

To test whether personality predicts cognitive domain scores, we will use four methods of data synthesis: 

## Method 1: Pooled Analysis of Individual Participant Data (IPD)  

Method 1 will consist of two methods: simple linear regression and linear regression with cluster robust standard errors.  

### Method 1A: Pooled Simple Linear Regression   
Method 1a estimates only an overall effect of personality on cognition by including all data in a single simple regression model.  

**Procedure**:  
1. Data across studies are cleaned and harmonized.  
2. All data are combined into a single data set.  
3. A single model tests the relationship between X & Y across all studies.  
$Y_{ij}=b_0+b_1*predictor_{ij} + \epsilon_{ij}$,  
where $b_1$ represents the overall effect of personality predicting the outcome.  

Models will be tested using the base `R` `lm()` function in `R`. Inferences will be made based on the 95\%  confidence intervals (CI).  

### Method 1B: Pooled Linear Regression with Cluster Robust Standard Errors  
Method 1b estimates only an overall effect of personality on cognition by including all data in a single  regression model, with cluster robust standard errors.  

**Procedure**:  
1. Data across studies are cleaned and harmonized.  
2. All data are combined into a single data set.  
3. A single model tests the relationship between X & Y across all studies.  
$Y_{ij}=b_0+b_1*predictor_{ij} + \epsilon_{ij}$, with study as a cluster,  
where $b_1$ represents the overall effect of personality predicting the outcome.  

Models will be tested using the `lm_robust()` function from the `estimatr` package in `R`. Inferences will be made based on the 95\% confidence intervals (CI).  

## Method 2: Pooled Analysis of Individual Participant Data using Dummy Codes or random effects 

Method 2 will consist of two separate methods: dummy codes and random effects.  

### Method 2A: Dummy Codes:  
For Method 2A, we will be estimating both an overall effect of personality-cognition relationships as well as study-specific estimates.  

**Procedure**:  
1. Data across all studies are cleaned and harmonized.  
2. All data are combined into a single data set.  
3. Effect codes are added to the data set for estimating differences in effects across studies.  
4. A single model tests the relationship between X & Y across studies, and effect codes provide an estimate of each studies deviation from the overall estimate.  
$Y_{ij}= b_0 + b_1*predictor_{ij} + b2*study1_{ij} + ... +b_k*studyk_{ij} + b_{k+1}*predictor_{ij}*study1_{ij} + ... + b_{2*k}*predictor_{ij}*studyk_{ij} + \epsilon_{ij}$,  
where k indicates the number of studies - 1. Of interest are two key sets of terms. $b_1$ indicates the average personality-cognition relationship, and $b_{k+1}$ to $b_{2k}$ represent effect coded study-specific differences in outcome associations (i.e. the estimate for a study is $b_1 + b_2k$). All other terms capture study-specific differences in overall cognitive ability levels, if any. There is no measure of cross-study heterogeneity.  

Models will be tested using the base `R` `lm()` function in `R`. Inferences will be made based on the 95\%  confidence intervals (CI).  

### Method 2B: Random Effects:  
For Method 2B, we will be estimating both an overall effect of personality-cognition relationships as well as study-specific estimates.  

**Procedure**:  
1. Data across all studies are cleaned and harmonized.  
2. All data are combined into a single data set.  
3. A single column indicates to which study each data point belongs.  
4. A single model tests the relationship between X & Y across studies, with random intercepts ($u_{0j}$) and slopes ($u_{1j}$) to capture study-specific deviations from the overall estimate ($\gamma_{10}$). The overall estimates for each study will be captured by $\beta_{1J}$, where j indicates each study 1 to j.   
$Y_{ij} = \beta_{0J} + \beta_{1J}*predictor_{ij} + \epsilon_{ij}$  
$\beta_{0J} = \gamma_{00} + u_{0j}$  
$\beta_{1J} = \gamma_{10} + u_{1j}$  

Models will be tested using the `lme4` packagage in `R`. Inferences will be made based on the 95\% confidence intervals (CI).  

## Method 3: Separate Analyses followed by random-effects meta-analysis
For Method 3, we will be estimating both an overall effect of personality-cognition relationships as well as study-specific estimates. However, unlike in Method 2, study-specific estimates will not be estimated simultaneously with overall estimates.    

**Procedure:**  
1. Data across all studies are cleaned. Harmonization optional but encouraged.  
2. All data remain in separate data sets.  
3. Separate models test the relationship between X & Y for each study.  
$Y_{ij} = b_0 + b_1 * predictor_{ij} + \epsilon_{ij}$  
4. Effect sizes are extracted from each model. ($b_1$)  
5. Results are pooled using random effects meta-analysis.  
$T_i=\mu + \zeta_i + \epsilon_i$,  
where $T_i$ is the study-specific effect of study $i$, $\mu$ is the overall meta-analytic estimate, $\zeta_i$ is true study variability of study $i$ from the overall estimate, and $\epsilon_i$ is sampling error.  

Study-specific models will be estimated using the Base `R` `lm()` function. Random effects meta analyses will be estimated using the `metafor` package in `R`. Inferences will be based off the 95\% confidence intervals (CI).  

## Method 4: Separate Analyses Reported Together  
1. Data across all studies are cleaned. Harmonization optional but encouraged.  
2. All data remain in separate data sets.  
3. Separate models test the relationship between X & Y for each study.  
$Y_{ij}=b_0 + b_1 * predictor_{ij} + \epsilon_{ij}$  
4. Effect sizes are extracted from each model.  
5. Results are not pooled and are presented together to demonstrate cross-study heterogeneity.  

Models will be tested using the base `R` `lm()` function in `R`. Inferences will be made based on the 95\%  confidence intervals (CI).  

**2. Do estimates of personality-crystallized / knowledge domain relationships vary across studies?**   
Not all of the data synthesis methods in the proposed study include inferential tests of cross-study heterogeneity. Thus, we will examine inferential statistics of cross-study heterogeneity only in Methods 2B and 3. In random effects meta-analysis (method 3), this can be examined using $I^2$, which estimates the percentage of the variability in effect estimates that is due to heterogeneity rather than sampling error (chance). In method 2B, this can be examined by looking at the Level 2 variances, specifically the variance of the personality-crystallized / knowledge domain ($\tau_{11}^2$) and calculating an effect-modified intraclass correlation, which is very similar to $I^2$ in meta-analyses estimates the percentage of variance that is due to cross-study heterogeneity rather than sampling error. ($\frac{tau_{11}^2}{\tau_{11}^2 + \sigma^2}$).  

## Statistical models
<!-- What statistical model will you use to test each hypothesis? Please include the type of model (e.g. ANOVA, multiple regression, SEM, etc) and the specification of the model (this includes each variable that will be included as predictors, outcomes, or covariates). Please specify any interactions, subgroup analyses, pairwise or complex contrasts, or follow-up tests from omnibus tests. If you plan on using any positive controls, negative controls, or manipulation checks you may mention that here. Remember that any test not included here must be noted as an exploratory test in your final article.

This is perhaps the most important and most complicated question within the preregistration. As with all of the other questions, the key is to provide a specific recipe for analyzing the collected data. Ask yourself: is enough detail provided to run the same analysis again with the information provided by the user? Be aware for instances where the statistical models appear specific, but actually leave openings for the precise test. See the following examples:

- If someone specifies a 2x3 ANOVA with both factors within subjects, there is still flexibility with the various types of ANOVAs that could be run. Either a repeated measures ANOVA (RMANOVA) or a multivariate ANOVA (MANOVA) could be used for that design, which are two different tests. 
- If you are going to perform a sequential analysis and check after 50, 100, and 150 samples, you must also specify the p-values you’ll test against at those three points.

Example:  We will use a one-way between subjects ANOVA to analyze our results. The manipulated, categorical independent variable is 'sugar' whereas the dependent variable is our taste index. -->

Statistical models will be simple linear regression, cluster robust linear regression, multilevel regression, and meta-analysis (a form of multilevel regression).  

Models will be run separately for each trait-cognition combination, both with and without background variables / covariates, but interpreted results will include all covariates. Unadjusted models will be included in Supplementary Materials.  

In addition, all results will further be tested using Bayesian regression. Corresponding analyses will be conducted using the the highly flexible `brms` package in `R`, which can capture all of the modeling approaches above, including meta-analyses. Given large sample sizes and relatively robust expected effect sizes, we do not expect these to greatly differ from frequentist estimates. These results will be presented in the Supplementary Materials.  

## Transformations
<!-- If you plan on transforming, centering, recoding the data, or will require a coding scheme for categorical variables, please describe that process. If any categorical predictors are included in a regression, indicate how those variables will be coded (e.g. dummy coding, summation coding, etc.) and what the reference category will be.

Example: The "Effect of sugar on brownie tastiness" does not require any additional transformations. However, if it were using a regression analysis and each level of sweet had been categorically described (e.g. not sweet, somewhat sweet, sweet, and very sweet), 'sweet' could be dummy coded with 'not sweet' as the reference category. -->

All data recoding, compositing, and transformations are meticulously documented in the codebook attached with this preregistration. This codebook will be directly read into `R` for the purposes of rescaling and recoding variables to ensure maximal transparency and reproducibility.  

Key transformations are:  
- Personality, cognitive ability, and self-rated health will be scaled as POMP across studies.  
- Age will be coded in years  
- Education will be coded in years  
- Gender will be coded as 0 = male, 1 = female  

## Inference criteria
<!-- What criteria will you use to make inferences? Please describe the information youÍll use (e.g. p-values, bayes factors, specific model fit indices), as well as cut-off criterion, where appropriate. Will you be using one or two tailed tests for each of your analyses? If you are comparing multiple conditions or testing multiple hypotheses, will you account for this?

p-values, confidence intervals, and effect sizes are standard means for making an inference, and any level is acceptable, though some criteria must be specified in this or previous fields. Bayesian analyses should specify a Bayes factor or a credible interval. If you are selecting models, then how will you determine the relative quality of each? In regards to multiple comparisons, this is a question with few "wrong" answers. In other words, transparency is more important than any specific method of controlling the false discovery rate or false error rate. One may state an intention to report all tests conducted or one may conduct a specific correction procedure; either strategy is acceptable.

Example: We will use the standard p<.05 criteria for determining if the ANOVA and the post hoc test suggest that the results are significantly different from those expected if the null hypothesis were correct. The post-hoc Tukey-Kramer test adjusts for multiple comparisons. -->

For frequentist tests, inferences will be based on 95\% bootstrapped confidence intervals. Intervals that do not overlap with 0 will be considered statistically significant.  

For Bayesian tests, inference will be based around the Bayesian 95\% credibility interval (CI) of the effect. Intervals that do not overlap with 0 will be considered statistically significant.  

## Data exclusion and missing data  
<!-- How will you determine what data or samples, if any, to exclude from your analyses? How will outliers be handled? Will you use any awareness check? Any rule for excluding a particular set of data is acceptable. One may describe rules for excluding a participant or for identifying outlier data.

Example: No checks will be performed to determine eligibility for inclusion besides verification that each subject answered each of the three tastiness indices. Outliers will be included in the analysis. -->

Every effort will be made to use the maximum amount of data possible. However, participants who are missing key personality, cognitive ability scores, or demographic variables will be dropped in analyses involving those variables. Where possible, they will still be included in other tests.  

Participants who have no record of any personality characteristic or cognition will be dropped completely.  

No imputation will be used.  

## Exploratory analyses (optional)
<!-- If you plan to explore your data set to look for unexpected differences or relationships, you may describe those tests here. An exploratory test is any test where a prediction is not made up front, or there are multiple possible tests that you are going to use. A statistically significant finding in an exploratory test is a great way to form a new confirmatory hypothesis, which could be registered at a later time.

Example: We expect that certain demographic traits may be related to taste preferences. Therefore, we will look for relationships between demographic variables (age, gender, income, and marital status) and the primary outcome measures of taste preferences. -->

We will examine a series of person and study-level moderators using Methods 2 and 3. 

Person-level:  

* gender  
* age (in years, centered within-study)  
* education (in years, centered at 12, i.e. secondary education)  

Study-Level:  

* continent of study  
* country of study  
* personality scale  
* interval between personality and cog scale  
* study baseline year  
* study baseline age  

# References
```{r create_r-references}
papaja::r_refs(file = "r-references.bib")
```

