--- 
title: "A Taxonomy of Data Synthesis: A Tutorial"
author: 
    
  - name        : Emorie D. Beck
    affiliation : Feinberg School of Medicine
  - name        : Emily C. Willroth
    affiliation : Feinberg School of Medicine
  - name        : Daniel K. Mroczek
    affiliation : Feinberg School of Medicine, Northwestern University
  - name        : Eileen K. Graham
    affiliation : Feinberg School of Medicine
date            : "`r Sys.setlocale('LC_TIME', 'C'); format(Sys.time(), '%d\\\\ %B %Y')`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: emoriebeck/data-synthesis-tutorial
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
editor_options: 
  chunk_output_type: console
---

```{r, echo = F}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, error = F)
options(knitr.kable.NA = '')
```

# Workspace  

In this section, we'll set up everything we need to clean data in the next section. This includes: 

1. Loading in all packages
2. Loading in the codebook
  + Setting up data frames for personality traits / well-being, outcomes, covariates, and moderators, so that we can more easily rename their short-hand names to production ready ones later
3. Loading in and rendering html tables of some descriptives, measures, etc. 

## Packages  

```{r packages}
library(psych)       # psychometrics
library(knitr)       # knit documents
library(kableExtra)  # formatted tables
library(brms)        # bayesian models
library(readxl)      # read excel files
library(haven)       # read spss files
library(estimatr)    # robust standard error regression
library(lme4)        # Frequentist MLM
library(broom.mixed) # summaries of models
library(bootpredictlme4)    # for calculating prediction intervals
library(effectsize)  # effect sizes for meta-analysis
library(metafor)     # Frequentist meta-analysis
library(rstan)       # bayes underpinnings
library(tidybayes)   # pretty bayes draws and plots
library(cowplot)     # Plotting and faceting
library(plyr)        # data wrangling
library(tidyverse)   # data wrangling
library(furrr)       # parallel purrr mapping

pkg <- c("psych","knitr","kableExtra","brms","readxl","haven","estimatr",
         "lme4","broom.mixed","bootpredictlme4","effectsize","metafor",
         "rstan","tidybayes","cowplot","plyr","tidyverse","furrr")

lapply(pkg[!pkg %in% rownames(installed.packages())], function(x) install.packages(x))

lapply(pkg, function(x) print(citation(x), bibtex = T))
lapply(pkg, function(x) print(paste(x, "version", packageVersion(x))))
```

## Directory Path  
```{r path}
# res_path <- "https://github.com/emoriebeck/big-five-prediction/blob/master"
data_path <- "/Volumes/Emorie/data"
res_path <- "https://github.com/emoriebeck/data-synthesis-tutorial/raw/main"
# local_path <- "/Volumes/Emorie/projects/data synthesis/crystallized"
local_path <- "~/Documents/projects/data synthesis/crystallized"
```

## Introduction  
A key part of the scientific enterprise involves establishing robust, replicable, and generalizable relationships among diverse phenomena. For the better part of a century, meta-analytic techniques, in which effect sizes of relationships among phenomena are pulled from the published or unpublished literature and statistically pooled, have been the cornerstone of testing the robustness and generalizability. However, as more data become publicly available, it is becoming increasingly easy and important to synthesize data sources, rather than just results of those sources. In response, a number of new techniques, including pooled analyses, individual participant meta-analyses, and coordinated analyses, have emerged to synthesize such diverse data sources.  

Despite the promises of each of these techniques, there has been little to no systematic review of the methods available or how to carry them out. As a result, many researchers are unaware of the wealth of methods available for data synthesis. But understanding what methods are available and how to best carry them out is critical for guiding future research using different data synthesis techniques. The present study aims to fill this gap.  

In addition, in recent years, the links between personality and cognitive ability and their links to aging have become increasingly popular as researchers look to use them to understand how aging processes unfold. However, less research has looked at links between personality and specific domains of cognitive functioning, particularly in a multi-study format, which is critical to understand how the interplay among personality, cognitive function, and aging unfold in a more nuanced manner.  

To demonstrate how to conduct a variety of data synthesis techniques, as well as their utility and challenges, the proposed study investigates whether the Big Five prospectively predicts crystallized / knowledge domain of cognitive ability in 13 longitudinal panel studies. Because of the many options available for synthesizing the data from these studies to test the association, we will detail five broad data synthesis methods: (1) pooled analysis of individual participant data (IPD), (2) pooled analysis of individual participant data (IPD) using dummy codes or random effects, (3) coordinated analyses followed by random effects meta-analysis, (4)  coordinated analyses reported together, and (5) traditional meta-analysis of effect sizes from the published and unpublished literature. In addition, we will demonstrate how to carry out four of these five methods (excluding traditional meta-analyses).  

Each of these methods will be explained in more detail later, but key features and differences across methods are summarized in Table 1 below.  

```{r data synth tables}
url <- "https://github.com/emoriebeck/data-synthesis-tutorial/raw/main/codebooks/crystallized_tables.xlsx"
destfile <- "tables.xlsx"
curl::curl_download(url, destfile)

tab1 <- readxl::read_xlsx(destfile, sheet = "Table 1") %>%
  select(-ModNum) %>%
  kable(.
        , "html"
        , align = c("r", "c", "l", "l", "c", "l")
        , caption = "<strong>Table 1</strong><br><em>Key Features of Five Levels of Data Synthesis</em>") %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  kableExtra::group_rows("Single Model", 1, 2) %>%
  kableExtra::group_rows("Multiple Models", 3, 5) 
tab1
save_kable(tab1, file = sprintf("%s/results/tables/tab-1-taxonomy.html", local_path))
```

```{r tab 2}
tab2 <- readxl::read_xlsx(destfile, sheet = "Sheet1") %>%
  mutate_all(~str_replace_all(., "\\r\\n", "<br>")) %>%
  kable(.
        , "html"
        , align = rep("c", 10)
        , caption = "<strong>Table 2</strong><br><em>Sample characteristics and sample-level moderators</em>"
        , escape = F
        , col.names = c("Sample", "Country (Continent)", "Prediction Interval", "Measure", "Scale", "Domains", 
                        "Median Year (SD)", "Baseline Age", "Measure(s)","Median Year (SD)")
        ) %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  add_header_above(c(" " = 3, "<strong>Personality Characteristics</strong>" = 5, "<strong>Crystallized / Knowledge Domain Cognitive Ability</strong>" = 2)
                     , escape = F) %>%
  footnote("E = Extraversion; A = Agreeableness; C = Conscientiousness; N = Neuroticism; O = Openness. NEO-FFI = 60 item NEO Five Factor Inventory (Costa & McCrae, 1992); IPIP NEO = International Item Pool in Personality NEO (Johnson, 2014); BFI-S = Big Five Inventory, Short Form (German; Hahn et al., 2012); TDA-40 = Trait Descriptive Adjectives-40 (Saucier, 1994); MIDI = The Midlife Development Inventory (Lachman & Weaver, 1997); DPQ = Dutch Personality Questionnaire (Barelds & Luteijn, 2002); Eysenck = Eysenck Personality Questionnaire (Eysenck & Eysenck, 1965). Prediction interval was calculated by taking each participantsâ€™ first personality measurement year from their last cognitive ability measurement year. Baseline age is the average participant age at their first personality assessment. ")
tab2
save_kable(tab2, file = sprintf("%s/results/tables/tab-2-samples.html", local_path))
```


## Codebook  

Each study has a separate codebook indexing matching, covariate, personality, and outcome variables. Moreover, these codebooks contain information about the original scale of the variable, any recoding of the variable (including binarizing outcomes, changing the scale, and removing missing data), reverse coding of scale variables, categories, etc.  

```{r codebook}
url <- "https://github.com/emoriebeck/data-synthesis-tutorial/raw/main/codebooks/crystallized_codebook_10.02.20.xlsx"
destfile2 <- "crystallized_codebook_10.02.20.xlsx"
curl::curl_download(url, destfile2)
# list of all codebook sheets
sheets <- excel_sheets(destfile2)

# function for reading in sheets
read_fun <- function(x){
  print(x)
  read_xlsx(destfile2, sheet = x)
}

# read in sheets and index source
codebook <- tibble(
  study = sheets,
  codebook = map(study, read_fun)
)

## short and long versions of names of all categories for later use
studies <- c("BASE-I", "CNLSY", "EAS", "GSOEP", "HILDA", "HRS", "LASA", "MAP", "MARS", "OCTO-TWIN", "ROS", "SATSA", "SLS")
studies_long <- c("BASE-I", "CNLSY", "EAS", "GSOEP", "HILDA", "HRS", "LASA", "MAP", "MARS", "OCTO-TWIN", "ROS", "SATSA", "SLS")
studies_sp   <- c("   BASE-I", "   CNLSY", "     EAS", "   GSOEP", "   HILDA", "     HRS", "    LASA", "     MAP", "    MARS", "OCTO-TWIN", "     ROS", "   SATSA", "     SLS")

traits <- codebook$codebook[[2]] %>% filter(category == "pers") %>% 
  select(long_name = Construct, short_name = name)

outcomes <- codebook$codebook[[2]] %>% filter(category == "outcome") %>%
  select(long_name = Construct, short_name = name)

covars <- codebook$codebook[[2]] %>% filter(category == "moder") %>%
  select(long_name = Construct, short_name = name, long_term = new_terms, short_term = old_terms)

moders <- covars %>% 
  mutate(long_name = mapvalues(long_name, "Unadjusted", "None"))

stdyModers <- codebook$codebook[[2]] %>% filter(category == "metaMod") %>%
  select(long_name = Construct, short_name = name, long_term = new_terms, medium_term = short_term, short_term = old_terms)

mthds <- codebook$codebook[[2]] %>% filter(Category == "Methods") %>%
  select(long_name = Construct, short_name = name, old_name = old_terms)

stdcolors <- tibble(
  studies = c("Overall", studies)
  , std_text = str_remove_all(studies, "[-]")
  , colors = c("black", "#332288", "#88ccee", "#44aa99", "#117733", "#999933", "#ddcc77", 
               "#cc6677", "#332288", "#88ccee", "#44aa99", "#117733", "#999933", "#ddcc77")
       , lt = c(rep("solid", 8), rep("dotted", 6)))

# used personality waves 
p_waves <- read_xlsx(destfile, sheet = "Table 2")
```

## Navigating This Tutorial  

This tutorial is meant to demonstrate how to accurately and efficiently carry out different methods of data synthesis. Therefore, each of the methods will be conducted separately in different chapters of this document and then the results of all will be pooled in the final chapter. Sometimes, this will result in redundant code. This is intentional such that should you want to conduct a one-stage pooled analysis of individual participant data using random effects, you would only need to follow the introduction, Chapter 1 on Data cleaning, and Chapter 3B on one-stage pooled analysis of individual participant data using random effects. Together, these would take you through every step from documenting your data to creating final tables and figures of your results without having to click through or find additional pieces in other chapters. At a later point, each of these may be separated into R scripts for each method to facilitate ease of use.  

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


